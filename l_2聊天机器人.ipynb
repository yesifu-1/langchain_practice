{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "958c9b75-712f-481e-b9e5-5e01cca874eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import getpass\n",
    "import os\n",
    "import langchain\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage,AIMessage\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "\n",
    "# 2. Create model\n",
    "model = AzureChatOpenAI(\n",
    "    azure_endpoint=\"https://12205-m2hl4tqk-eastus.openai.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2024-08-01-preview\",\n",
    "    azure_deployment=\"gpt-35-turbo\",\n",
    "    openai_api_version=\"2024-08-01-preview\",\n",
    "    api_key=\"d7f27353b3b3463bb02b2708df922f35\",  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5a5f2cc-297e-4987-a9d7-2bded49ee5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=StrOutputParser()\n",
    "chain=model|parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beebf67-9e2b-4862-b1ec-c2e602d41d19",
   "metadata": {},
   "source": [
    "### 首先直接使用模型。ChatModel是LangChain“运行接口”的实例，这意味着它们提供了一个标准接口供我们与之交互。要简单地调用模型，我们可以将消息列表传递给.invoke方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a98a6d3-292a-4f8f-8b3c-ed361c8f5b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[HumanMessage(content='i am ed'),SystemMessage(content='answer word<10')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71e2dcb5-27fe-44e0-9f99-1badee4d5cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778acf58-64ec-4f85-9f61-f4bd8d997fb4",
   "metadata": {},
   "source": [
    "### 模型本身没有任何状态概念。例如，如果问一个后续问题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91821be6-a570-4bcc-8475-ef2f03ac93d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I don't know your name. Could you please tell me?\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_2=[HumanMessage(content='what is my name'),SystemMessage(content='answer word<10')]\n",
    "chain.invoke(messages_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2eb2bd-4568-4aab-8eb6-3c72a63b2255",
   "metadata": {},
   "source": [
    "### 为了绕过这个问题，我们需要将整个对话历史\n",
    "### 传递给模型。让我们看看这样做会发生什么："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2091e7dc-0180-47d6-aeb7-4d2a5bff58ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is jack_five.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm jack_five\"),\n",
    "        AIMessage(content=\"HI !!!\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da24b09-479f-4dce-8b9b-2891e7808bea",
   "metadata": {},
   "source": [
    "### 现在得到了好的回应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f5de5-6ad7-4b0a-a3b3-635089a77a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8deadb1f-389f-4ab7-b115-a5707f3ee7e4",
   "metadata": {},
   "source": [
    "# 消息历史"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dc15cb-a945-4ff3-93a1-8a1b657bb2e1",
   "metadata": {},
   "source": [
    "#### 我们可以使用消息历史类来包装我们的模型，使其具有状态。 这将跟踪模型的输入和输出，并将其存储在某个数据存储中。 未来的交互将加载这些消息，并将其作为输入的一部分传递给链。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0da2ed46-ae43-4aa1-9264-93f5961a5141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#我们可以使用消息历史类来包装我们的模型，使其具有状态。 这将跟踪模型的输入和输出，\n",
    "#并将其存储在某个数据存储中。 未来的交互将加载这些消息，并将其作为输入的一部分传递给链。\n",
    "\n",
    "#之后，我们可以导入相关类并设置我们的链，该链包装模型并添加此消息历史。这里的一个关键部分是我们作为 get_session_history 传入的函数。这个函数预计接受一个 session_id 并返回一个消息历史对象。这个 session_id 用于区分不同的对话，并应作为配置的\n",
    "#一部分在调用新链时传入（我们将展示如何做到这一点）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2010bca-0f70-471b-9b43-69b294b31a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "206c2fa3-340e-4ebf-b78e-f4a378e318ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}#用于存储会话 ID 和对应的 BaseChatMessageHistory 对象之间的映射。每个 session_id 都对应一个 BaseChatMessageHistory 实例。\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "# session_id: str：函数接收一个字符串参数 session_id，表示会话的唯一标识符。\n",
    "# -> BaseChatMessageHistory：函数的返回类型是 BaseChatMessageHistory，表示该函数将返回一个 \n",
    "# BaseChatMessageHistory 类型的对象。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f72ccd26-be4b-4408-95be-4e747d3454d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)\n",
    "#新的模型，拥有了对话历史函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4f264b-ca6e-443d-a037-869a40d1e2c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 我们现在需要创建一个 config，每次都传递给可运行的部分。这个配置包含的信息并不是直接作为输入的一部分，但仍然是有用的。在这种情况下，我们想要包含一个 session_id。这应该看起来像："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1524fccf-f4e5-4f5d-900a-000c8c06e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c850e652-5e71-412b-8bdb-9b207bc83bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Bob\")],\n",
    "    config=config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b362fad0-a0b3-46ed-abba-8f41d14e5c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Bob! How can I assist you today?'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a37177a1-1368-4841-b7da-0d28302ec9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Bob.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d9c9cb-ea4d-49b7-8c16-1365397578cb",
   "metadata": {},
   "source": [
    "#### 我们的聊天机器人现在记住了关于我们的事情。如果我们更改配置以引用不同的 session_id，我们可以看到它开始新的对话。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fc0f85bb-642b-4e09-93c4-3775110cada6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, as an AI language model, I don't have access to your personal information. Please introduce yourself.\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc3\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231c7cbb-16b2-412b-909c-b9cd547a9b22",
   "metadata": {},
   "source": [
    "#### 这就是我们如何支持聊天机器人与多个用户进行对话的方式！现在，我们所做的只是为模型添加了一个简单的持久化层。我们可以通过添加提示词模板来使其变得更加复杂和个性化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86a10f2-2c3d-4f8b-b631-42d7c8b17da3",
   "metadata": {},
   "source": [
    "#### 提示词模板帮助将原始用户信息转换为大型语言模型可以处理的格式。在这种情况下，原始用户输入只是一个消息，我们将其传递给大型语言模型。现在让我们使其变得更复杂一些。首先，让我们添加一个带有一些自定义指令的系统消息（但仍然将消息作为输入）。接下来，我们将添加除了消息之外的更多输入。首先，让我们添加一个系统消息。为此，我们将创建一个 ChatPromptTemplate。我们将利用 MessagesPlaceholder 来传递所有消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cef730ba-4803-4a99-a20d-d2ad8246e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3127745-8930-4a2e-81f1-afaf3ab8ab07",
   "metadata": {},
   "source": [
    "#### 注意，这稍微改变了输入类型 - 我们现在传递的是一个包含 messages 键的字典，其中包含一系列消息，而不是传递消息列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4700fc0a-77fb-43b7-8d3e-7c1bd820a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content='hi!i am bob')]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8444c94d-b8c1-4ac3-8162-464fee9bc6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Bob! How can I assist you today?'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab4514c-24c4-4123-908b-1be2178446c4",
   "metadata": {},
   "source": [
    "### 我们现在可以将其包装在与之前相同的消息历史对象中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7d3f2139-53e9-4b73-82c3-611a50b9f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9a3c5dbe-7884-46dd-898e-116f8841d20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc4\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "152b5d60-7b12-4d87-9e4d-cf7cf9a4d33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Jim! How can I assist you today?'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Jim\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7398405a-9ae1-41e1-9bc6-17ef129e072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 从下面的输入可以看到，在MessagesPlaceholder(variable_name=\"messages\")\n",
    "#的位置输入多个HumanMessage，虽然先输入的HumanMessage没有应答，但也保存在了\n",
    "#store中 可以用于后续的回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "594775fa-6e4e-403c-a118-57958fa47cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If you\\'re looking for a great work of fiction, I would highly recommend \"The Great Gatsby\" by F. Scott Fitzgerald. The novel is set in the roaring 1920s and follows the wealthy and mysterious Jay Gatsby as he tries to win back his lost love, Daisy Buchanan. The book is a classic and is known for its beautiful prose, vivid imagery, and exploration of themes such as love, wealth, and the American Dream.'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"my brother'name is wang\"),HumanMessage(content=\"introduce me a fiction book\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "897515a3-964d-4086-8a64-847e3df77f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your brother's name is Wang.\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"what is my brother'name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87c88e-2740-4a74-a979-fd62b6985e89",
   "metadata": {},
   "source": [
    "### 现在让我们使我们的提示变得更复杂一点。假设提示模板现在看起来像这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ea687cc6-d05a-4056-ab8a-d745658981c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "78d1ed7e-12c1-4a88-b0e4-8150ac569ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Hola Bob! ¿En qué puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! I'm bob\")], \"language\": \"Spanish\"}\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f644ba2-08d9-49f4-a2e4-18e32d52b601",
   "metadata": {},
   "source": [
    "### 现在让我们将这个更复杂的链封装在一个消息历史类中。这次，由于输入中有多个键，我们需要指定正确的键来保存聊天历史。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6ef9590f-d2f3-452f-8cca-5804724bc436",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "587623c3-dee9-4937-b2ab-745776efa6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc11\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5806c70d-9c13-4399-8700-7c8e002e806a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hola Todd, ¿cómo puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! I'm todd\")], \"language\": \"Spanish\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e176821-7ebd-4264-9b59-40a02e6fd809",
   "metadata": {},
   "source": [
    "# 编写自定义令牌计数器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dc2189-a81c-4c0b-9197-6c15ee311901",
   "metadata": {},
   "source": [
    "### 编写一个自定义令牌计数器函数，该函数接受消息列表并返回一个整数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0b0cb081-1ec7-4099-98a5-adcdec66d7ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "# pip install tiktoken\n",
    "import tiktoken\n",
    "from langchain_core.messages import BaseMessage, ToolMessage\n",
    "\n",
    "\n",
    "def str_token_counter(text: str) -> int:\n",
    "    enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "\n",
    "def tiktoken_counter(messages: List[BaseMessage]) -> int:\n",
    "    \"\"\"Approximately reproduce https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "\n",
    "    For simplicity only supports str Message.contents.\n",
    "    \"\"\"\n",
    "    num_tokens = 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    tokens_per_message = 3\n",
    "    tokens_per_name = 1\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            role = \"user\"\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            role = \"assistant\"\n",
    "        elif isinstance(msg, ToolMessage):\n",
    "            role = \"tool\"\n",
    "        elif isinstance(msg, SystemMessage):\n",
    "            role = \"system\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported messages type {msg.__class__}\")\n",
    "        num_tokens += (\n",
    "            tokens_per_message\n",
    "            + str_token_counter(role)\n",
    "            + str_token_counter(msg.content)\n",
    "        )\n",
    "        if msg.name:\n",
    "            num_tokens += tokens_per_name + str_token_counter(msg.name)\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "trim_messages(\n",
    "    messages,\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=tiktoken_counter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eaa965-a22e-40ab-96dc-803514106401",
   "metadata": {},
   "source": [
    "## 管理对话历史"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1a921-2764-4277-a201-3680f5567c5a",
   "metadata": {},
   "source": [
    "#### 构建聊天机器人时，一个重要的概念是如何管理对话历史。如果不加以管理，消息列表将无限增长，并可能溢出大型语言模型的上下文窗口。因此，添加一个限制您传入消息大小的步骤是很重要的。\n",
    "#### 重要的是，您需要在提示模板之前但在从消息历史加载之前的消息之后执行此操作。\n",
    "#### 我们可以通过在提示前添加一个简单的步骤，适当地修改 messages 键，然后将该新链封装在消息历史类中来实现。\n",
    "#### LangChain 提供了一些内置的助手来 管理消息列表。在这种情况下，我们将使用 trim_messages 助手来减少我们发送给模型的消息数量。修剪器允许我们指定希望保留的令牌数量，以及其他参数，例如是否希望始终保留系统消息以及是否允许部分消息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5f43c8dc-e07d-4e5d-b93b-3995af803198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=60,    \n",
    "    strategy=\"last\",#last or first     last是从后向前保留消息，会丢较前的输入\n",
    "    token_counter=tiktoken_counter, #令牌计数器\n",
    "    include_system=True,   #保留系统消息\n",
    "    allow_partial=False, #指定是否允许部分修剪。如果设置为 True，即使无法完全满足 max_tokens 的要求，也会返回尽可能多的消息；如果设置为 F\n",
    "    #alse，则只有在能够完全满足 max_tokens 的情况下才会返回消息，否则返回空列表。\n",
    "    start_on=\"human\", #指定从哪种类型的消息开始修剪\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d78bfd-7da5-48e6-9783-5d94ab749217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3110e4-6182-4ebe-b873-dcd793ad56ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fcade9-ffda-471a-affb-b87940558faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48315b0f-1dd5-4845-b520-283c70c92411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004f387a-94ab-4c86-abd4-0d8b20e35a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmer = trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm,\n",
    "    include_system=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03248e30-8a6f-4962-a03d-41badb571b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3efdee-26aa-42fa-9c0f-e121585e417b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e9418010-f464-4ab9-a765-ec0a5f2800a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'startswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 26\u001b[0m\n\u001b[0;32m      3\u001b[0m trimmer \u001b[38;5;241m=\u001b[39m trim_messages(\n\u001b[0;32m      4\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m65\u001b[39m,\n\u001b[0;32m      5\u001b[0m     strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m#从最后一条消息开始向前修剪，直到总 token 数量不超过 max_tokens\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     start_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#指定从哪种类型的消息开始修剪 human ai 还是system\u001b[39;00m\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     13\u001b[0m     SystemMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre a good assistant\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     14\u001b[0m     HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi! I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm bob\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     AIMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes!\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     24\u001b[0m ]\n\u001b[1;32m---> 26\u001b[0m \u001b[43mtrimmer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#要在我们的链中使用它，我们只需在将 messages 输入传递给提示之前运行修剪器。\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#现在如果我们尝试询问模型我们的名字，它将不知道，因为我们修剪了聊天历史的那部分：\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:4713\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   4699\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[0;32m   4700\u001b[0m \n\u001b[0;32m   4701\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4710\u001b[0m \u001b[38;5;124;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[0;32m   4711\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 4713\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m   4714\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke,\n\u001b[0;32m   4715\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   4716\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config(config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc),\n\u001b[0;32m   4717\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4718\u001b[0m     )\n\u001b[0;32m   4719\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4720\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4721\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4722\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4723\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:1927\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   1923\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1924\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1925\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1926\u001b[0m         Output,\n\u001b[1;32m-> 1927\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1928\u001b[0m             call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1929\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1930\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1931\u001b[0m             config,\n\u001b[0;32m   1932\u001b[0m             run_manager,\n\u001b[0;32m   1933\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1934\u001b[0m         ),\n\u001b[0;32m   1935\u001b[0m     )\n\u001b[0;32m   1936\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1937\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:4567\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   4565\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[0;32m   4566\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4567\u001b[0m     output \u001b[38;5;241m=\u001b[39m call_func_with_variable_args(\n\u001b[0;32m   4568\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   4569\u001b[0m     )\n\u001b[0;32m   4570\u001b[0m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[0;32m   4571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\messages\\utils.py:869\u001b[0m, in \u001b[0;36mtrim_messages\u001b[1;34m(messages, max_tokens, token_counter, strategy, allow_partial, end_on, start_on, include_system, text_splitter)\u001b[0m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _first_max_tokens(\n\u001b[0;32m    861\u001b[0m         messages,\n\u001b[0;32m    862\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    866\u001b[0m         end_on\u001b[38;5;241m=\u001b[39mend_on,\n\u001b[0;32m    867\u001b[0m     )\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 869\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_last_max_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_counter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlist_token_counter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_partial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_partial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_system\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_system\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_splitter_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrategy\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m. Supported strategies are \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\messages\\utils.py:1305\u001b[0m, in \u001b[0;36m_last_max_tokens\u001b[1;34m(messages, max_tokens, token_counter, text_splitter, allow_partial, include_system, start_on, end_on)\u001b[0m\n\u001b[0;32m   1302\u001b[0m swapped_system \u001b[38;5;241m=\u001b[39m include_system \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(messages[\u001b[38;5;241m0\u001b[39m], SystemMessage)\n\u001b[0;32m   1303\u001b[0m reversed_ \u001b[38;5;241m=\u001b[39m messages[:\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m messages[\u001b[38;5;241m1\u001b[39m:][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m swapped_system \u001b[38;5;28;01melse\u001b[39;00m messages[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m-> 1305\u001b[0m reversed_ \u001b[38;5;241m=\u001b[39m \u001b[43m_first_max_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreversed_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_counter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_counter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_splitter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartial_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlast\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mallow_partial\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m swapped_system:\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reversed_[:\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m reversed_[\u001b[38;5;241m1\u001b[39m:][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\messages\\utils.py:1222\u001b[0m, in \u001b[0;36m_first_max_tokens\u001b[1;34m(messages, max_tokens, token_counter, text_splitter, partial_strategy, end_on)\u001b[0m\n\u001b[0;32m   1220\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(messages)):\n\u001b[1;32m-> 1222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtoken_counter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_tokens:\n\u001b[0;32m   1223\u001b[0m         idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(messages) \u001b[38;5;241m-\u001b[39m i\n\u001b[0;32m   1224\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:950\u001b[0m, in \u001b[0;36mBaseChatOpenAI.get_num_tokens_from_messages\u001b[1;34m(self, messages, tools)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m:\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mget_num_tokens_from_messages(messages)\n\u001b[1;32m--> 950\u001b[0m model, encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_encoding_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-0301\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;66;03m# every message follows <im_start>{role/name}\\n{content}<im_end>\\n\u001b[39;00m\n\u001b[0;32m    953\u001b[0m     tokens_per_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:904\u001b[0m, in \u001b[0;36mBaseChatOpenAI._get_encoding_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    902\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 904\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[43mtiktoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding_for_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    906\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl100k_base\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\langchain\\lib\\site-packages\\tiktoken\\model.py:105\u001b[0m, in \u001b[0;36mencoding_for_model\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoding_for_model\u001b[39m(model_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Encoding:\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the encoding used by a model.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    Raises a KeyError if the model name is not recognised.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_encoding(\u001b[43mencoding_name_for_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\langchain\\lib\\site-packages\\tiktoken\\model.py:88\u001b[0m, in \u001b[0;36mencoding_name_for_model\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# Check if the model matches a known prefix\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# Prefix matching avoids needing library updates for every model version release\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# Note that this can match on non-existent models (e.g., gpt-3.5-turbo-FAKE)\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model_prefix, model_encoding_name \u001b[38;5;129;01min\u001b[39;00m MODEL_PREFIX_TO_ENCODING\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 88\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m(model_prefix):\n\u001b[0;32m     89\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m model_encoding_name\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'startswith'"
     ]
    }
   ],
   "source": [
    "#要在我们的链中使用它，我们只需在将 messages 输入传递给提示之前运行修剪器。\n",
    "\n",
    "#现在如果我们尝试询问模型我们的名字，它将不知道，因为我们修剪了聊天历史的那部分：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a440ba51-8629-4259-88f4-1ec7e511739c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "91d70705-e341-4935-812d-6b975d3b28af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[114], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel_name\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_name' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
